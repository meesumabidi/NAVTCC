{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmmrEqDZSnIMw1ladQCbC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meesumabidi/NAVTCC/blob/main/sirasim_whatsappchat_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTymQyIXdx5_",
        "outputId": "33d4ec60-5209-45a0-cd36-7db0f925f78f"
      },
      "source": [
        "pip install emoji\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "AjCkTUHJdlK2",
        "outputId": "24018e2b-b961-43a2-8b12-df6014e07867"
      },
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from nltk import *\n",
        "from datetime import date\n",
        "import plotly.express as px\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import text2emotion as te\n",
        "\n",
        "# Extract Time\n",
        "def date_time(s):\n",
        "    pattern = '^([0-9]+)(\\/)([0-9]+)(\\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'\n",
        "    result = re.match(pattern, s)\n",
        "    if result:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Find Authors or Contacts\n",
        "def find_author(s):\n",
        "    s = s.split(\":\")\n",
        "    if len(s)==2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Finding Messages\n",
        "def getDatapoint(line):\n",
        "    splitline = line.split(' - ')\n",
        "    dateTime = splitline[0]\n",
        "    date, time = dateTime.split(\", \")\n",
        "    message = \" \".join(splitline[1:])\n",
        "    if find_author(message):\n",
        "        splitmessage = message.split(\": \")\n",
        "        author = splitmessage[0]\n",
        "        message = \" \".join(splitmessage[1:])\n",
        "    else:\n",
        "        author= None\n",
        "    return date, time, author, message\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "data = []\n",
        "conversation = 'whatsapp chat.txt'\n",
        "with open(conversation, encoding=\"utf-8\") as fp:\n",
        "    fp.readline()\n",
        "    messageBuffer = []\n",
        "    date, time, author = None, None, None\n",
        "    while True:\n",
        "        line = fp.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if date_time(line):\n",
        "            if len(messageBuffer) > 0:\n",
        "                data.append([date, time, author, ' '.join(messageBuffer)])\n",
        "            messageBuffer.clear()\n",
        "            date, time, author, message = getDatapoint(line)\n",
        "            messageBuffer.append(message)\n",
        "        else:\n",
        "            messageBuffer.append(line)\n",
        "\n",
        "\n",
        "# # unsupervised learning \n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Date', 'Time', 'Author', 'Message'])\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df\n",
        "# data = df.dropna()\n",
        "# # data\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# sentiments = SentimentIntensityAnalyzer()\n",
        "# data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"Message\"]]\n",
        "# data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"Message\"]]\n",
        "# data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"Message\"]]\n",
        "# print(data.head())\n",
        "\n",
        "\n",
        "# # creating new columns year month day time hour minute weeday\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "df['year'] = df['Date'].dt.year\n",
        "df['month'] = df['Date'].dt.month_name()\n",
        "df['day'] = df['Date'].dt.day\n",
        "df['Time'] = pd.to_datetime(df['Time'])\n",
        "df['hour'] = df['Time'].dt.hour\n",
        "df['minute'] = df['Time'].dt.minute\n",
        "df['weekday'] = df['Date'].dt.day_name()\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "# # # split URL, letter and word count \n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "\n",
        "#column urlcount\n",
        "URLPATTERN = r'(https?://\\S+)'\n",
        "df['urlcount'] = df.Message.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n",
        "\n",
        "#column Letter_Count\n",
        "df['Letter_Count'] = df['Message'].apply(lambda s : len(s))\n",
        "#column Word_Count\n",
        "df['Word_Count'] = df['Message'].apply(lambda s : len(s.split(' ')))\n",
        "df.head()\n",
        "\n",
        "\n",
        "# # extract emoji in message\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "\n",
        "def split_count(text):\n",
        "    emoji_list = []\n",
        "    data = re.findall(u'[\\U0001f300-\\U0001f650]|[\\u2000-\\u3000]', text)\n",
        "#     data = re.findall(u'(?:\\uD83C[\\uDF00-\\uDFFF])|(?:\\uD83D[\\uDC00-\\uDDFF])', text)\n",
        "    \n",
        "#     print(data)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
        "            emoji_list.append(word)\n",
        "#             print(emoji_list)\n",
        "    return emoji_list\n",
        "\n",
        "df[\"emoji\"] = df[\"Message\"].apply(split_count)\n",
        "df.head(50)\n",
        "\n",
        "\n",
        "# # chart shows the number of messages on weekdays and weekends (Time Series)\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "date_grouped = df.groupby('weekday')['Message'].count().plot(kind='line', figsize=(20,10), color='#A26360')\n",
        "\n",
        "\n",
        "# # chart shows the number of messages date wise\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "date_grouped = df.groupby('Date')['Message'].count().plot(kind='line', figsize=(20,10), color='#A26360')\n",
        "\n",
        "\n",
        "# # favorite days to chat with friends\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "weekday_grouped_msg =  (df.set_index('weekday')['Message']\n",
        "                          .groupby(level=0)\n",
        "                          .value_counts()\n",
        "                          .groupby(level=0)\n",
        "                          .sum()\n",
        "                          .reset_index(name='count'))\n",
        "weekday_grouped_msg\n",
        "\n",
        "fig = px.line_polar(weekday_grouped_msg, r='count', theta='weekday', line_close=True)\n",
        "fig.update_traces(fill='toself')\n",
        "fig.update_layout(\n",
        "  polar=dict(\n",
        "    radialaxis=dict(\n",
        "      visible=True,\n",
        "    )),\n",
        "  showlegend=False\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# # Total Messages per hour\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "hour_grouped_msg =  (df.set_index('hour')['Message']\n",
        "                          .groupby(level=0)\n",
        "                          .value_counts()\n",
        "                          .groupby(level=0)\n",
        "                          .sum()\n",
        "                          .reset_index(name='count'))\n",
        "fig = px.bar(hour_grouped_msg, x='hour', y='count',\n",
        "                 Labels={'hour':'24 Hour Period'}, \n",
        "                 height=400)\n",
        "fig.update_traces(marker_color='#EDCC8B', marker_line_color='#D4A29C',\n",
        "                  marker_line_width=1.5, opacity=0.6)\n",
        "fig.update_layout(title_text='Total Messages by Hour of the Day')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# # pivot table day wise and month wise\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "grouped_by_month_and_day = df.groupby(['month', 'weekday'])['Message'].value_counts().reset_index(name='count')\n",
        "grouped_by_month_and_day\n",
        "months= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "pt = grouped_by_month_and_day.pivot_table(index= 'month', columns= 'weekday', values='count').reindex(index=months, columns= days)\n",
        "fig = px.imshow(pt, Labels=dict(x=\"Day of Week\", y=\"Months\", color=\"Count\"), x=days y=months)\n",
        "fig.update_layout(\n",
        "    width = 700, height = 700)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# # which author sent maximum messages \n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "qty_message_author = df['Author'].value_counts()\n",
        "qty_message_author.plot(kind='barh',figsize=(20,10), color=['#D4A29C', '#E8B298', '#EDCC8B', '#BDD1C5', '#9DAAA2'])\n",
        "qty_message_author.head(10)\n",
        "\n",
        "\n",
        "# # find common words and their frequency\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "commond_words = df[['Author','Message']].copy()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = stopwords.words('english')\n",
        "\n",
        "stopwords = list(STOPWORDS)\n",
        "extra = [\"<multimedia\", \"omitido>\", \"k\", \"d\",\"si\",\"multimedia\", \"omitido\"]\n",
        "stopwords = stopwords + extra\n",
        "commond_words[\"Message\"] = (commond_words[\"Message\"]\n",
        "                           .str.lower()\n",
        "                           .str.split()\n",
        "                           .apply(lambda x: [item for item in x if item not in stopwords])\n",
        "                           .explode()\n",
        "                           .reset_index(drop=True)\n",
        "                 )\n",
        "\n",
        "# commond_words['Message']= commond_words['Message'].apply(remove_emoji)\n",
        "commond_words['Message']= commond_words['Message'].replace('nan', np.NaN)\n",
        "commond_words['Message']= commond_words['Message'].replace('', np.NaN)\n",
        "commond_words['Message']= commond_words.Message.str.replace(r\"(a|j)?(ja)+(a|j)?\", \"jaja\",regex=True)\n",
        "commond_words['Message']= commond_words.Message.str.replace(r\"(a|j)?(jaja)+(a|j)?\", \"jaja\",regex=True)\n",
        "\n",
        "\n",
        "words_dict = dict(Counter(commond_words.Message))\n",
        "words_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "words_dict = pd.DataFrame(words_dict, columns=['words', 'count'])\n",
        "\n",
        "fig = px.bar(words_dict.head(10).dropna(), x='words', y='count',\n",
        "                 Labels={'words':'Common Words'}, \n",
        "                 height=400)\n",
        "fig.update_traces(marker_color='#EDCC8B', marker_line_color='#D4A29C',\n",
        "                  marker_line_width=1.5, opacity=0.6)\n",
        "fig.update_layout(title_text='Commond Words Chart')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# In[35]:\n",
        "\n",
        "\n",
        "#function to display wordcloud\n",
        "def plot_cloud(wordcloud):\n",
        "    # Set figure size\n",
        "    plt.figure(figsize=(40, 30))\n",
        "    # Display image\n",
        "    plt.imshow(wordcloud) \n",
        "    # No axis details\n",
        "    plt.axis(\"off\");\n",
        "\n",
        "#function to remove urls from text\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "# # word cloud\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "chat_word_cloud = df[['Message']].copy()\n",
        "# chat_word_cloud['Message']= chat_word_cloud['Message'].apply(remove_emoji)\n",
        "chat_word_cloud['Message']= chat_word_cloud['Message'].apply(remove_urls)\n",
        "chat_word_cloud['Message']= chat_word_cloud['Message'].replace('nan', np.NaN)\n",
        "chat_word_cloud['Message']= chat_word_cloud['Message'].replace('', np.NaN)\n",
        "chat_word_cloud['Message']= chat_word_cloud.Message.str.replace(r\"(a|j)?(ja)+(a|j)?\", \"jaja\",regex=True)\n",
        "chat_word_cloud['Message']= chat_word_cloud.Message.str.replace(r\"(a|j)?(jaja)+(a|j)?\", \"jaja\",regex=True)\n",
        "text = \" \".join(review for review in chat_word_cloud.Message.dropna())\n",
        "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, \n",
        "background_color='black', colormap='Set2', collocations=False,\n",
        "stopwords = stopwords).generate(text)\n",
        "# Plot\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[38]:\n",
        "\n",
        "\n",
        "# df = pd.DataFrame(data, columns=[\"Date\", 'Time', 'Author', 'Message'])\n",
        "# df['Date'] = pd.to_datetime(df['Date'])\n",
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "data = df.dropna()\n",
        "\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"Message\"]]\n",
        "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"Message\"]]\n",
        "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"Message\"]]\n",
        "\n",
        "print(data.head(50))\n",
        "\n",
        "\n",
        "# In[43]:\n",
        "\n",
        "\n",
        "for i in data[\"Message\"]:\n",
        "    te.get_emotion(i)\n",
        "text = \"I was asked to sign a third party contract a week out from stay. If it wasn't an 8 person group that took a lot of wrangling I would have cancelled the booking straight away. Bathrooms - there are no stand alone bathrooms. Please consider this - you have to clear out the main bedroom to use that bathroom. Other option is you walk through a different bedroom to get to its en-suite. Signs all over the apartment - there are signs everywhere - some helpful - some telling you rules. Perhaps some people like this but It negatively affected our enjoyment of the accommodation. Stairs - lots of them - some had slightly bending wood which caused a minor injury.\"\n",
        "te.get_emotion(text)\n",
        "# sentiments.polarity_scores(i)[\"pos\"] for i in data[\"Message\"]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-67fe0e839049>\"\u001b[0;36m, line \u001b[0;32m217\u001b[0m\n\u001b[0;31m    fig = px.imshow(pt, Labels=dict(x=\"Day of Week\", y=\"Months\", color=\"Count\"), x=days y=months)\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2kenDHEeaK2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}